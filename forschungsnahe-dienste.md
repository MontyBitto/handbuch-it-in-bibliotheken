---
description: Dienste wie Repositorien und Forschungsdatenmanagement zur Unterstützung von Forschungsprozessen
---

# Forschungsnahe Dienste {#forschungsnahe-dienste}

::: {.callout-important}

Dieses Kapitel befindet sich in einer ersten öffentliche Entwurfsversion. Feedback siehe Links im rechten Menü und [Hinweise zur Mitarbeit](mitarbeit.md).

:::

::: {.callout-note}
## Zusammenfassung

Bibliotheken bieten [Dienste für die Wissenschaft](#einleitung) im Rahmen des
digitalen Wandels von Publikationsprozessen und Open Science an. Dazu gehören
[Dienste zur Publikation](#publikationsdienste), zur [Verwaltung von
Forschungsdaten](#forschungsdatenmanagement) und
[Forschungssoftware](#forschungssoftware) sowie [Informationen über
Forschungsprozesse](#forschungsinformationssysteme).

*[Übergreifende Themen](#übergreifende-themen) wie diese [Zertifikate und
Standards](#zertifikate-und-standards) und [zusammen mit externen
Dienstleistern](#zusammenarbeit-mit-dienstleistern)  werden in diesem Kapitel
auch behandelt, der Text muss aber noch überarbeitet werden!*

:::

## Einleitung

Unter forschungsnahen Diensten werden verschiedene Bibliotheksservices
zusammengefasst, die Wissenschaftler\*innen im gesamten
Forschungsprozess unterstützen sollen und die überwiegend im Kontext von
digitalem Wandel und Open Science angesiedelt sind
(<https://www.o-bib.de/bib/article/view/5718>).
Dazu zählen z. B. Services in den Bereichen [Forschungsdatenmanagement],
Bibliometrie sowie verschiedene [Publikationsdienste].

Einige dieser Dienste, z. B. Repositorien für Zeitschriftenartikel, die
unter Open-Access-Bedingungen zweitveröffentlicht werden dürfen, gehören
schon seit Jahrzehnten zum Dienstleistungsrepertoire wissenschaftlicher
Bibliotheken. Inzwischen betreiben größere Einrichtungen zudem oft
spezialisierte Repositorien für ein Spektrum verschiedener Objekttypen:
Publikationen, Zeitschriften, Bücher, Open Educational Resources,
Forschungsdaten, Forschungssoftware und mehr. Dazu gehören auch diverse
Dienste, die übergreifend den Forschungsoutput bestimmter lokaler oder
fachlicher Forschungscommunitys besser auffindbar oder messbar machen
sollen, darunter Forschungsinformationssysteme und Dienstleistungen im
Bereich von Metriken oder zur Verwaltung von
Artikelveröffentlichungsgebühren in Open-Access-Journals (APCs).

Forschungsförderer und Ministerien erwarten in Deutschland heute von den
Forschungseinrichtungen eine Transformation des Publizierens hin zu Open
Access und Open Data. Dazu gehören themenbezogene institutionelle
Policys, Beratungs- und Schulungsangebote sowie technische Dienste.
Unter anderem auf der Ebene der europäischen Forschungsförderung wird
versucht, durch Initiativen wie *[CoARA](https://coara.eu/)*
die Maßstäbe der Forschungsbewertung weiterzuentwickeln - weg von
klassisch bibliometrischen Indikatoren wie h-Index und
Journal-Impact-Faktor hin zu einer Würdigung vielfältiger
verschiedenartiger Forschungsergebnisse.

In Abgrenzung zur Dominanz von Großverlagen ist im Bereich der
wissenschaftlichen Informationsinfrastrukturen darüber hinaus der
Anspruch entstanden, Dienste - bis hin zur Ebene von Entwicklung und
Betrieb zugehöriger IT-Infrastrukturen - durch die Communitys der
Forschenden selbst zu betreiben. In der englischsprachigen
Fachdiskussion sind dazu Begriffe wie "scholar-led publishing" oder
"scholar-led conferences" geprägt worden, vgl. exemplarisch hierzu das
[Scholar-Led.Network Manifesto](https://graphite.page/scholar-led-manifesto/).
Bibliotheken sehen sich hier als vertrauenswürdige, öffentlich
finanzierte Dienstleister für die Wissenschaftscommunitys.

Eine Konsequenz daraus ist, dass es bei den forschungsnahen Diensten so
selbstverständlich wie in wohl keinem anderen Bereich der IT an
Bibliotheken ist, Open Source-Software einzusetzen und oft sogar aktiv
an Open Source-Software mitzuentwickeln. Auch die Bedeutung der
gemeinschaftlichen Pflege offener Standards, Datenmodelle und
Daten-Gemeingüter ist nur in wenigen Bereichen bibliothekarischer
Dienstleistungen so ausgeprägt wie hier.

## Publikationsdienste

### Journal Publishing-Dienste 

Open Access bedeutet, dass wissenschaftliche Literatur kostenfrei und
öffentlich im Internet zugänglich ist, sodass Interessierte die
Volltexte lesen, herunterladen, kopieren, verteilen, drucken, in ihnen
suchen, auf sie verweisen und sie auch sonst auf jede denkbare legale
Weise benutzen können, ohne finanzielle, gesetzliche oder technische
Barrieren jenseits von denen, die mit dem Internetzugang selbst
verbunden sind. Zum ersten Mal wurde dieser Gedanke in der
[Grundsatzerklärung der Budapester
Open-Access-Bewegung](https://www.budapestopenaccessinitiative.org/translations/german-translation)
formuliert.

Durch die Transformationsprozesse im wissenschaftlichen
Publikationswesen weg von den traditionellen Abonnement-Modellen hin zu
Open Access sehen sich Bibliotheken zunehmend auch in der Rolle eines
Publikationsdienstleisters. Dies kann einerseits die Gründung eines
Universitätsverlages bedeuten, andererseits aber auch die Schaffung der
benötigten Infrastruktur für wissenschaftliche Zeitschriften. Befördert
durch die Verlagerung der Kosten weg vom Lesen hin zum Publizieren wird
der *scholar-led*-Ansatz immer gefragter und Bibliotheken geraten in die
Situation, Expertise in diesem Bereich aufbauen zu müssen.

Zur Schaffung einer technischen Infrastruktur lässt sich z. B. mit der Software
*[Open Journal Systems (OJS)](https://ojs-de.net/ueber-ojs)* eine Plattform zur
Verfügung stellen, die die strukturierte Veröffentlichung von
Zeitschriften(-artikeln) ermöglicht. Parallel dazu müssen auch die
erforderlichen Abläufe und Organisationsstrukturen angepasst werden.
Personelle Ressourcen müssen hier ebenso bedacht werden. In erster Linie gilt
es, die Herausgeber\*innen-Teams der Zeitschriften zu unterstützen.
Gleichzeitig sollte die Bibliothek auch technischen Support für einreichende
Autor\*innen bieten. Der Funktionsumfang von OJS ermöglicht es auch, einen
Workflow für den Peer-Review-Prozess abzubilden. Auch hier liegt Potenzial für
die Unterstützung durch Bibliotheken. Wichtig ist somit ein Überblick über den
Gesamtprozess des wissenschaftlichen Publizierens und nicht nur die
Software-Aspekte.

Parallel dazu entwickeln sich derzeit \"alternative
Publikations-Plattformen" wie Preprint-Dienste, (Micro-)Blogs, Data
Journals und ähnliche Dienste, die traditionelle Publikationswege wie
peer-reviewte Journals ergänzen, siehe dazu auch die entsprechende
[Publikation der Initiative Knowledge
Exchange](https://doi.org/10.21428/996e2e37.3ebdc864). Hier
können Bibliotheken ebenso in die Rolle des Dienstleisters treten,
Instanzen geeigneter Open-Source-Software hosten und diese Entwicklung
unterstützen. Dies kann auch im Zusammenhang mit der externen
Kommunikation von Bibliotheken (Link zu entsprechendem Kapitel)
betrachtet werden. Ergänzend gibt es zahlreiche weitere Dienste, die das
Open-Access-Publizieren vor allem administrativ unterstützen, von denen
im Folgenden einige kurz erläutert werden.

### Open-Access-Dienste

Neben der Unterstützung bei Open-Access-Veröffentlichungen haben sich im
Zusammenhang mit der Open-Access-Transformation weitere IT-Dienste in
Bibliotheken entwickelt.

#### Verlags-Software

Da im Sinne der "scholar-led infrastructure" zunehmend
Universitätsverlage gegründet werden, existieren hier auch eigene
Software-Lösungen für die Verwaltung der Titel, Lagerbestände,
Kund\*innen-, Adress- und Versanddaten. Häufig ist ein eigener Webshop
mit entsprechenden Shopfunktionalitäten integriert und es gibt
Schnittstellen zur Buchhaltung und Auslieferung. Für kleinere Verlage
reicht vermutlich ein normaler Webshop aus; Spezialsoftware kann
verlagsspezifische Anforderungen allerdings besser abbilden. Betrieb und
Administration der Programme gehören mit in das zugehörige
Diensteportfolio.

#### Publikationsfonds-Verwaltung

Publikationsfonds zur Finanzierung von Open-Access-Publikationen sind an
vielen wissenschaftlichen Bibliotheken fest verankerte Hilfsmittel, um
die Transformation des wissenschaftlichen Publikationswesens hin zu Open
Access zu unterstützen. Durch sie werden anfallende Article Processing
Charges (APC) nach durch die Einrichtung vorgegebenen Kriterien
(mindestens anteilig) zentral gezahlt. Zum Monitoring der entstehenden
Kosten ist häufig eine Tabellenkalkulationssoftware ausreichend. Für
einen Vergleich verschiedener Einrichtungen und ein einheitliches
Reporting existieren jedoch Services wie
[OpenAPC](https://openapc.net/). Zur einheitlichen
Darstellung aller Publikationskosten hat sich mit
[OpenCost](https://www.opencost.de/metadatenschema/) ein
xml-Metadatenschema etabliert.

#### Open Refine

Sollen Daten aus unterschiedlichen Quellen zusammengeführt werden oder
ein Abgleich gegen externe Datenbanken stattfinden, ist
*[OpenRefine](https://openrefine.org/)* ein geeignetes
Werkzeug: Über offene Schnittstellen können Datensätze sowohl erweitert
als auch beispielsweise in Wikidata exportiert werden. Hierbei ist keine
zentrale Installation notwendig. Die Software wird lokal auf den
Endgeräten ausgeführt. Lediglich eine korrekte Einstellung der Firewall,
um die Kommunikation mit externen Datenbanken zu ermöglichen, gilt es zu
beachten.

#### Journal-Finder-Werkzeuge

Neben den üblichen Fragen zu Lizenzen bei Open-Access-Publikationen
spielt auch die Wahl eines geeigneten Journals eine zentrale Rolle.
Derzeit verbreitete Journal-Finder-Werkzeuge wie
[B!SON](https://service.tib.eu/bison/) oder der
[oa.finder](https://finder.open-access.network/) greifen
für die Journal-Daten zwar beide auf das Directory of Open Access
Journals ([DOAJ](https://doaj.org/)) zu, verfolgen jedoch
unterschiedliche Ansätze. Der oa.finder zeigt eine Liste von
Zeitschriften mit Filteroptionen und den jeweiligen Förderbedingungen
an. Diese werden aus bestehenden Transformationsverträgen abgeleitet und
enthalten keine spezifischen Förderbedingungen. B!SON bietet
teilnehmenden Einrichtungen die Möglichkeit, eine eigene Liste mit
Förderbedingungen zu hinterlegen. Treffer werden direkt mit den
korrekten Förderbedingungen angezeigt. Zu diesem Zweck muss ein
Web-Backend entsprechend der Vertragssituation gepflegt werden.

### Repositorien für Forschungsergebnisse

Zentral für die Veröffentlichung jedweder Art von wissenschaftlichem
Output ist ein geeigneter Ort für deren Veröffentlichung, ganz besonders
im Hinblick auf die zunehmende Datengetriebenheit der Wissenschaften.

Heute haben sich Open-Access-Repositorien als verlässliche Speicherdienste für
wissenschaftliche Ergebnisse etabliert, seien es [Forschungsdaten], textuelle
Publikationen oder andere Materialien. Für die wissenschaftliche Kommunikation
ist dabei die Zitierbarkeit eine wichtige Voraussetzung. Dazu muss
sichergestellt werden, dass publizierte Ergebnisse in der veröffentlichten Form
erhalten werden, das heißt nicht verändert oder gelöscht werden. Da
Internetadressen als flüchtig gelten, werden zur Identifikation [Persistent
Identifier-Systeme](#persistent-identifier) eingesetzt. Die Zitierfähigkeit und
damit der langfristige, möglichst originalgetreue Erhalt der einmal
eingestellten Informationen grenzen Repositorien gegenüber anderen
Arbeitsplattformen wie Sync-and-Share-Plattformen (z.  B. Nextcloud),
Content-Management-Systemen (zur Erstellung von Blogs und Internetseiten) sowie
virtuellen Forschungsumgebungen (mit integrierten Funktionen z. B. für die
Datenanalyse) ab.

Es lassen sich grundlegend zwei Typen von Repositorien unterscheiden:
Disziplinspezifische Repositorien sammeln die Inhalte einer bestimmten
Forschungsdisziplin (Suche nach Disziplin möglich über
[re3data](https://www.re3data.org/)), während generische
Repositorien Inhalte unterschiedlicher Disziplinen aufnehmen (z. B.
[Zenodo](https://zenodo.org)). Einen Sonderfall
fachübergreifender Repositorien bilden institutionelle Repositorien, die
speziell für die Mitglieder der jeweiligen Institution zur Verfügung
stehen. Einige Repositorien widmen sich zudem speziell der Sammlung
bestimmter Datentypen (z. B. Preprint- oder Publikationsserver,
Forschungsdatenrepositorien, Bilddatenbanken, ...). Insbesondere im
Bereich Forschungsdaten und Open Educational Resources gibt es seitens
der Nutzer\*innen häufig den Wunsch, dass in Repositorien auch eine
interaktive Arbeit mit den Materialien möglich ist. Derzeit sind
Repositorien aber in der Regel noch ausschließlich auf die sichere
Speicherung und Bereitstellung der Inhalte ausgerichtet - eine
Erweiterung um Funktionalitäten z. B. zur Visualisierung oder Analyse
würden sie in die Nähe von virtuellen Forschungsumgebungen rücken.
Häufig werden Repositorien und Repositoriensoftware zudem mit der
digitalen Langzeitarchivierung als Kernfunktionalität in Verbindung
gebracht - aus Sicht von Repositorien-Betreiber\*innen und
-Entwickler\*innen handelt es sich bei der digitalen
Langzeitarchivierung allerdings eher um eine Spezialfunktionalität, die
nur für bestimmte Nutzungsszenarien relevant und daher auch nicht in
allen Repositorien gegeben ist. Mit dem proprietären System
[Rosetta](https://exlibrisgroup.com/de/produkte/rosetta/)
und der Open Source-Lösung
[Archivematica](https://www.archivematica.org) seien hier
nur zwei Beispiele für Systeme genannt, die sich auf den Erhalt von
Informationen im Sinne der Langzeitarchivierung spezialisieren.

::: {.callout-important}
## Definition

Der Begriff **Repository** bzw. **Repositorium** wird in diesem Kapitel
vorwiegend verwendet für Plattformen, in denen Forschungsergebnisse
dauerhaft archiviert, beschrieben, auffindbar und zugänglich gemacht
werden. Darüber hinaus wird der Begriff Repository aber auch für
Versionsverwaltungssysteme wie Git verwendet, die vorwiegend von
Softwareprojekten genutzt werden, um es einer geschlossenen oder offenen
Community zu ermöglichen, transparent und konfliktfrei zur Codebasis des
jeweiligen Projekts beizutragen. Unter anderem
[Zenodo](https://www.zenodo.org) erlaubt über eine
Schnittstelle zum Code-Verwaltungssystem GitHub die Archivierung von
Softwareprojekten und ähnlichem entsprechend den Konventionen von
Repositorien für Forschungsergebnisse, einschließlich der Vergabe von
DOIs.

:::

Das *Open Directory of Open Access Repositories (OpenDOAR)* stellt
[Statistiken über die Verbreitung von
Softwarelösungen](https://v2.sherpa.ac.uk/view/repository_visualisations/1.html)
für Repositorien für Forschungsergebnisse bereit: Meistgenutztes System
weltweit ist *[DSpace](https://www.dspace.org)*, das auch in
Deutschland zunehmend Verbreitung findet. Während DSpace vor allem von
Universitäten eingesetzt wird, ist bei Fachhochschulen und HAWen
*[OPUS](https://www.opus-repository.org/)* stark verbreitet,
das meist durch den Kooperativen Bibliotheksverbund Berlin-Brandenburg
(KOBV) oder das Bibliotheksservice-Zentrum Baden-Württemberg (BSZ)
gehostet wird. Der Gemeinsame Bibliotheksverbund (GBV) bietet mit
*[MyCORE](http://www.mycore.org)* eine weitere
Repositoriensoftwarelösung an. OPUS und MyCORE finden bislang
ausschließlich im deutschsprachigen Raum Anwendung, während DSpace von
einer globalen Community getragen wird. Für Forschungsdatenrepositorien
kommt zum Teil DSpace, zum Teil die auf Forschungsdaten spezialisierte
Software *[Dataverse](https://dataverse.org)* zum Einsatz.

TODO: Abb. zu den Informationstypen im Zusammenhang mit Repositorien? Etwas:

![BILDUNTERSCHRIFT FEHLT](media/information-types.png)

Referenzen

- Beate Rajski, Pascal Becker, \"DSpace-Konsortium Deutschland --
  konsortial die Nachhaltigkeit sichern\". *LIBREAS. Library Ideas*, 36
  (2019). <https://libreas.eu/ausgabe36/rajski/>

- [Repositorien und das Semantic Web](https://doi.org/10.14279/depositonce-5015)

## Forschungsdatenmanagement

[Forschungsdaten]: #forschungsdatenmanagement

Die digitale Transformation hat Forschungsprozesse grundlegend
verändert: In zahlreichen Fachdisziplinen entstehen an
Forschungseinrichtungen heute täglich große Mengen digitaler Daten, die
als Forschungsgegenstand dienen, angereichert, analysiert oder
visualisiert werden. Dabei stehen Wissenschaftler\*innen vor der
Herausforderung, diese Daten nicht nur zu verwalten, sondern sie auch
langfristig und nachvollziehbar vorzuhalten und möglichst offen zur
Nachnutzung zur Verfügung zu stellen. Maßgeblich sind hierfür die
sogenannten **FAIR-Prinzipien**, denen zufolge Forschungsdaten auffindbar
(findable), zugänglich (accessible), interoperabel (interoperable) und
nachnutzbar (reusable) sein sollen (vgl.
[https://doi.org/10.1038/sdata.2016.18](https://doi.org/10.1038/sdata.2016.18)
).

::: {.callout-important}
## Definition

**Forschungsdaten** sind alle digital vorliegenden Daten, die während des
Forschungsprozesses entstehen (z. B. Messdaten, Laborwerte, Videoaufnahmen,
Umfrageergebnisse). Klar davon abzugrenzen sind
[Forschungsinformationen](#forschungsinformationssysteme).

:::

Die Schnittmenge von Forschungsdaten und Forschungsinformationen liegt wie in
@fig-fis-fdm dargestellt im Bereich Publikationen. Während sich allerdings
Forschungsinformationen eher auf klassische, kontrolliert publizierte Dokumente
beziehen, geht die Publikation von Forschungsdaten weit darüber hinaus und
schließt jegliche Form von Aufzeichnungen wie Notizen und Zwischenergebnisse
bis zu [Forschungssoftware](#forschungssoftware) mit ein.

![Forschungsinformationen und ihre Sicht auf Forschungsdaten](media/FIS_FDM_CC_BY_Mau.png){#fig-fis-fdm}

Der Lebenszyklus von Forschungsdaten beinhaltet die Erstellung,
Speicherung, Archivierung bis hin zur Löschung aussortierter Daten.

Services zum Forschungsdatenmanagement sollen Wissenschaftler\*innen
beim Umgang mit ihren Forschungsdaten unterstützen, und zwar über den
gesamten Forschungsdatenlebenszyklus hinweg, d. h. von der Datenplanung
über die Datenerhebung und -analyse bis hin zur Datenarchivierung,
-publikation und -nachnutzung. Bibliotheken nehmen beim Aufbau und
Betrieb entsprechender Services eine zentrale Rolle ein - in aller Regel
sind sie hierbei jedoch nicht die einzigen Akteure, sondern das
Serviceportfolio wird in einer arbeitsteiligen Zusammenarbeit von
Bibliotheken, Rechenzentren, Forschungsabteilungen und ggf. weiteren
Akteur\*innen angeboten. Angesichts der großen Heterogenität an
(disziplinspezifischen) Datentypen gelangen diese in aller Regel
fachübergreifenden FDM-Dienste allerdings häufig an ihre Grenzen: Diese
Erkenntnis ist konstitutiv für die seit 2020 im Aufbau befindliche
Nationale Forschungsdateninfrastruktur
([NFDI](https://www.nfdi.de/)), in der fachspezifische,
institutionsübergreifende Dienste entwickelt werden. In diesem
Zusammenhang entwickeln sich derzeit auch neue Berufe wie Data Steward
oder Data Librarian, die fachspezifische Unterstützung beim
Forschungsdatenmanagement leisten und entweder zentral an den
FDM-Servicestellen oder dezentral in Projekten oder Fachbereichen
angesiedelt sind.

Die von Bibliotheken angebotenen Services zum Forschungsdatenmanagement
umfassen in der Regel sowohl nicht-technische Services (z. B. Schulungs-
und Beratungsangebote) als auch verschiedene technische Dienste. Zu den
wichtigsten technischen Diensten für das Forschungsdatenmanagement, die
von Bibliotheken (mit-)betrieben werden, gehören
Forschungsdatenrepositorien. Diese ermöglichen die Veröffentlichung von
Forschungsdaten als eigene Informationsobjekte gemäß den FAIR-Prinzipien
(TODO: siehe hierzu ausführlich den Abschnitt Repositorien für
Forschungsergebnisse). Daneben werden häufig weitere FDM-Tools
angeboten, von denen einige im Folgenden vorgestellt werden sollen.

![BILDUNTERSCHRIFT FEHLT](media/fd-lifecycle.svg)

### Tools zur Erstellung von Datenmanagementplänen

Um den Umgang mit Forschungsdaten über ein komplettes Projekt zu
beschreiben, hat sich der **Datenmanagementplan (DMP)** als geeignetes
Format erwiesen. Derartige Pläne werden zunehmend von
Forschungsförderern bei der Antragstellung oder in der Frühphase des
Projekts erwartet und basieren häufig auf für die Förderlinie bzw. das
Fachgebiet spezifischen Fragenkatalogen. Durch die einheitlichen
Fragelisten und fest definierte Ausgabeformate lässt sich so ein
menschen- und maschinenlesbares Dokument generieren. Um diese
Fragenkataloge einheitlich zur Verfügung zu stellen und ggf. mit
einrichtungs- oder programmspezifischen Daten anzureichern, wurden
bereits einige Software-Werkzeuge entwickelt. In Deutschland verbreitet
ist der aus einem DFG-Projekt entstandene [Research Data Management
Organizer](https://rdmorganiser.github.io/) (RDMO), für
den, auch unterstützt durch NFDI-Konsortien, ständig neue Fragenkataloge
in einer Gemeinschaftsarbeit entwickelt werden. Obwohl einige
öffentliche Instanzen der Software existieren, die z. B. einen Login
über die ORCID ermöglichen, kann die Open-Source-Software auch selbst
gehostet und inhaltlich sowie visuell auf die Bedarfe der jeweiligen
Einrichtung zugeschnitten werden. DMP können innerhalb dieser Software
auch kollaborativ erstellt werden, indem Personen als Mitarbeitende in
das eigene Projekt eingeladen werden. Für EU-Projekte ist mit Stand 2023
die Software [ARGOS](https://argos.openaire.eu/)
verfügbar, die eine direkte Einbindung der DMP in die
[EOSC](https://eosc-portal.eu/) ermöglicht. Für Software
als Forschungsdatum beginnen sich Softwaremanagementpläne zu etablieren.

### Elektronische Laborbücher

Die Dokumentation der Forschungsergebnisse ist ein zentraler Punkt im
Forschungsdaten-Lebenszyklus. Digital entstandene Daten sollten ohne
Medienbruch mit ihrer Dokumentation verknüpft und zugehörige Metadaten
erfasst werden. Hierzu gibt es eine Reihe dedizierter Software-Lösungen,
die unter dem Begriff elektronische Laborbücher (abgekürzt häufig *ELN*
von *electronic laboratory notebook*) zusammengefasst werden. Neben
kommerziellen Programmen wie
[LabFolder](https://labfolder.com/de/), bei denen
Bibliotheken eher in der Rolle des Vermittlers sind, gewinnen
Open-Source-Lösungen zunehmend an Bedeutung. Diese können Bibliotheken
auf eigenen Servern hosten und selbst administrieren, sind jedoch bei
der Entwicklung neuer Features auf eine Community bzw. den Entwickler
oder eigene Fachkräfte angewiesen. Beispiele sind hier
[eLabFTW](https://www.elabftw.net/) für generische ELN
bzw. [Chemotion](https://chemotion.net/) für eine eher
fachspezifische Lösung. Eine [Handreichung zur Einführung eines
ELN](https://doi.org/10.17192/bfdm.2023.5.8553) an der
eigenen Einrichtung wurde 2023 von einer einrichtungsübergreifenden
Autor\*innengruppe erstellt. Open-Source-ELN erfahren häufig auch
Unterstützung durch NFDI-Konsortien. Der auch daraus entstandene Wunsch
eines einheitlichen Transferformats der Laborbucheinträge und
gemeinsamer Spezifikationen wird im [ELN
Consortium](https://github.com/TheELNConsortium)
adressiert.
Hilfestellung bei der Auswahl eines passenden Produkts bietet z. B. der
[ELN-Finder](https://eln-finder.ulb.tu-darmstadt.de/home).

### Git 

Als freie Software zur Versionsverwaltung ist Git ein Standardtool der
Softwareentwicklung geworden. Durch einfache Befehle auf der
Kommandozeile oder zusätzlich installierte Software mit grafischem
Interface lassen sich textuelle Daten auf dem eigenen System mit einem
externen (Code-)Repositorium abgleichen, das die notwendigen Protokolle
versteht. Neben dem großen Anbieter GitHub gibt es die lokal zu
installierende Software GitLab, um ein solches Repositorium in der
eigenen IT-Infrastruktur bereitzustellen. Durch die im Protokoll
integrierte Versionskontrolle der Daten lassen sich Änderungen im Code
einfach nachvollziehen und gegebenenfalls zurückrollen. Kollaborative
Arbeit in verteilten Teams zum Beispiel wird über eine parallele
Entwicklungsstruktur in "branches" ermöglicht, die mit dem Hauptprojekt
zu einem gewünschten Zeitraum zusammengeführt werden können. Eine
Möglichkeit, die Funktion der Software spielerisch zu erkunden, bietet
beispielsweise die Website
[ohmygit.org](https://ohmygit.org), ein unter anderem vom
Bundesministerium für Bildung und Forschung gefördertes Projekt.

*[Forschungssoftware](#forschungssoftware) lässt sich, nicht nur wegen der
Verwaltung mit Git oder ähnlichen Programmen, nicht einfach komplett analog zu
den Forschungsdaten behandeln, sondern bedarf eines genaueren Blicks.*

## Forschungssoftware

Bei der Betrachtung von Forschungsprozessen setzt sich zunehmend die
Erkenntnis durch, dass auch die dabei zum Einsatz kommende Software ein
Teil der Forschungsdaten ist. Dies ist häufig kein kommerziell
erhältliches Produkt, sondern ein speziell auf das Forschungsproblem
zugeschnittener, selbst programmierter Code. Wird derartige Software
nicht korrekt gesichert, versioniert und dokumentiert, leidet die
Reproduzierbarkeit von Forschungsdaten. Komplexe externe Probleme wie
prekäre Beschäftigungsverhältnisse können zusätzlich zum Verwaisen von
Softwareprojekten führen, wenn diese nur lokal durch einzelne engagierte
Personen vorangetrieben wurden. Zusätzlich führt eine Veröffentlichung
der Forschungssoftware zu einer Auffindbarkeit und Zitierbarkeit, so
dass diese zum wissenschaftlichen Output der Forschenden einen
signifikanten Beitrag leisten kann. Aus der Wissenschaft getriebene
Vereinigungen wie [de-RSE e. V.](https://de-rse.org/)
treten als Vereinszweck für den Stellenwert von Forschungssoftware ein.
Auch die FAIR-Prinzipien sollten für Forschungssoftware Anwendung finden
(vgl.
[https://doi.org/10.1038/s41597-022-01710-x](https://doi.org/10.1038/s41597-022-01710-x)).
Hier liegt es auch an den Bibliotheken, einerseits ein Bewusstsein dafür
zu schaffen (z. B. durch dedizierte Policies), aber andererseits auch
die benötigte Infrastruktur bereitzustellen.

Zur Zitierbarkeit von Forschungssoftware/Code dient die Generierung von
entsprechenden Metadaten, etwa über
[https://codemeta.github.io/](https://codemeta.github.io/)
und
[CITATION.cff](https://citation-file-format.github.io/),
um menschen- und maschinen-lesbare Zitierinformationen für Software und
Datensätze angeben zu können.

*TODO INFOBOX: CITATION.cff des Handbuchs als Beispiel*

Die Bereitstellung eines Coderepositoriums, in der Regel über Git,
sollte zum Standardangebot zählen. Für eine interaktive
wissenschaftliche Datenauswertung bietet sich zusätzlich der Betrieb
eines *[JupyterHub](https://jupyter.org/hub)* an. Dieser
ermöglicht die Nutzung von Jupyter Notebooks auf einem zentralen Server
der Einrichtung und ist so nicht abhängig von den jeweiligen
Rechenleistungen der verfügbaren Endgeräte. Zusätzlich sollte geprüft
werden, inwiefern eine Archivierung der Software bzw. eines gesamten
Repositoriums in der eigenen Infrastruktur nötig und sinnvoll ist.
Anbieter wie [Software
Heritage](https://www.softwareheritage.org/) bieten eine
Archivierung auf externen Servern an. Zumindest in diesem Fall ist es
als UNESCO-Projekt aber als geeignete Alternative zu betrachten.

Die Verknüpfung der im Gesamtprozess entstehenden Metadaten mit Systemen
wie dem FIS oder auch Forschungsdatenrepositorien hinsichtlich der
Auffindbarkeit dieser wissenschaftlichen Ergebnisse ist ein Punkt, der
eine Betrachtung der kompletten [Toolchain](#toolchain) notwendig macht.

## Forschungsinformationssysteme

::: {.callout-important}
## Definition

**Forschungsinformationen** Angaben über Aktivitäten, Ergebnisse und
Infrastrukturen von Forschungsprozessen wie zum Beispiel Projekte,
Publikationen und Forschungseinrichtungen. Davon zu Unterscheiden sind
[Forschungsdaten].

:::

Neben Forschungsdaten gewinnt auch die strukturierte Erfassung von
Forschungsinformationen an Bedeutung. Entsprechende Systeme werden
**Forschungsinformationssysteme** (FIS) genannt. Dabei handelt es sich um
Datenbanksysteme, die speziell für die Erfassung, Organisation,
Speicherung und Verknüpfung von **Forschungsinformationen**
konzipiert wurden. Sie können interne Anwendungen wie die
leistungsorientierte Mittelvergabe unterstützen, aber auch für die
Außendarstellung der Einrichtung genutzt werden. Eine Übersicht von
Forschungsinformationen und ihre Sicht auf Forschungsdaten gibt @fig-fis-fdm.

FIS führen Informationen zusammen, die dezentral in verschiedenen
hochschulinternen Systemen (z. B. Drittmittelverwaltung,
Personalverwaltungssysteme, Repositorien) und externen Quellsystemen (z.
B. Scopus, ORCID) vorgehalten werden, um einen strukturierten und
aktuellen Überblick über die Forschungsleistungen z. B. einer
Einrichtung, eines (Bundes-)Landes oder einer Fachdisziplin zu gewinnen.

Die genauen Daten, die Nutzung der Daten und der Funktionsumfang eines
FIS sind nicht festgelegt bzw. klar definiert. Verschiedene
Softwarelösungen verfolgen unterschiedliche Ansätze: Einige legen den
Schwerpunkt auf die Auffindbarkeit und Verknüpfung von Forschenden,
andere Systeme haben ihren Schwerpunkt eher auf dem Berichtswesen und
Monitoring und ggf. darauf basierenden Anreizsystemen. Wieder andere
Systeme legen den Schwerpunkt darauf, die Forschungsaktivitäten zu
präsentieren und öffentlichkeitswirksam bereitzustellen. Die Systeme
passen sich zunehmend aneinander an, oft werden verschiedene Systeme
aber auch in Kombination miteinander eingesetzt.

FIS sollten von Anfang an als Daueraufgabe einer Einrichtung betrachtet
und entsprechende finanzielle und personelle Ressourcen eingeplant
werden. Bei der Einführung eines FIS handelt es sich um ein langjähriges
Organisationsentwicklungsprojekt, das eine Offenheit für Veränderungen
in den Prozessen und Workflows der Einrichtung voraussetzt.

![BILDUNTERSCHRIFT FEHLT](media/FIS_CC_BY_Mau.png)

Eine zentrale Herausforderung beim Aufbau eines FIS besteht darin, einen
Überblick über die bestehenden Quellsysteme der Einrichtung zu gewinnen.
In diesem Zusammenhang ist zum einen zu ermitteln, welche internen und
externen Systeme relevant sind und wer die entsprechenden
Ansprechpartner\*innen an der Einrichtung sind. Dies betrifft u. a. die
Bibliothek (z. B. Repositorien), die Personalverwaltung
(Identitätsmanagement), die Drittmittelverwaltung (Datenbank für
Projekte), die Doktorand\*innenverwaltung oder die Patentverwaltung der
Einrichtung.

Neben der Identifikation der relevanten Datenquellen stellt die
Integration der Daten in das FIS meist die größte Herausforderung dar.
So muss zum einen für fehlende oder ungeeignete Schnittstellen eine
Lösung gefunden werden. Zum anderen variieren Qualität und Konsistenz
der vorhandenen Daten mitunter stark, was zusätzliche Zeit für die
Datenbereinigung und -konvertierung erfordert. Gleichzeitig ist die
Sicherstellung der Datenintegrität und -qualität von entscheidender
Bedeutung, um zu gewährleisten, dass das FIS korrekte und
aussagekräftige Informationen liefert.

Der Markt für FIS-Software ist sehr dynamisch. Vor dem Hintergrund, dass sich
gerade viele Forschungseinrichtungen in der Planungs- und Aufbauphase von
Forschungsinformationssystemen befinden, kommen in Deutschland immer neue
Softwarelösungen zum Einsatz. Es zeigt sich ein vielgestaltiges Bild aus
kommerziellen Produkten (z. B. *PURE*, *Converis*, HISinOne-RES), Open
Source-Lösungen (z. B. *DSpace-CRIS*, *VIVO*) und Eigenentwicklungen. An
deutschen Forschungseinrichtungen wird mittlerweile häufig *HISinOne-RES*
genutzt - befördert unter anderem durch Landesinitiativen wie *CRIS.NRW*,
*HeFIS* oder *FIS-Thüringen* sowie den Umstand, dass es aktuell das einzige
Produkt am Markt ist, dessen Datenmodell direkt am **Kerndatensatz Forschung
(KDSF)** ausgerichtet ist.  Obwohl sich ein Rückgang an Eigenentwicklungen
andeutet, sind sie immer noch weit verbreitet. Des Weiteren gibt es die bereits
lange etablierten kommerziellen Systeme *Converis* und *PURE*. Der Einsatz von
Open Source-Lösungen wie *DSpace-CRIS* und *VIVO* nimmt erst in den letzten
Jahren merklich zu -- unter anderem befördert durch das Verbundprojekt Hamburg
Open Science.

An vielen Einrichtungen besteht das Bestreben, dass das FIS zusätzlich auch die
Funktionalität eines [Repositoriums](#repositorien-für-forschungsergebnisse)
übernehmen soll. Ein Vorteil eines solchen vereinigten Systems wird zum einen
in den geringeren Systemkosten gesehen - zum anderen erscheint es weniger
aufwendig, die bibliographischen Einträge in einem FIS schlicht mit den
dazugehörigen Dateien anzureichern statt einen Workflow für das Zusammenspiel
zwischen FIS und Repositorium zu entwickeln. Dem entgegen stehen jedoch die
verschiedenen Zielsetzungen beider Systeme: Während es bei einem FIS vor allem
darum geht, möglichst alle Forschungsaktivitäten z.B. einer Einrichtung in
einem System zu erfassen, steht bei einem Repositorium die nachhaltige
Bereitstellung der Ressourcen selbst im Vordergrund (z.  B. textuelle
Publikationen oder Forschungsdaten). Ein Problem bei Mischsystemen ergibt sich
auch hinsichtlich Retrieval und Zugriff: So werden Forschende bei einer Suche
in externen Suchmaschinen z. B. erst im FIS feststellen, dass nur bei einem
Teil der Treffer tatsächlich Zugang zu den Ressourcen selbst besteht, in den
meisten Fällen jedoch lediglich Nachweise der Ressourcen finden. In der Praxis
sind FIS-Repositorien-Mischsysteme derzeit dennoch aufgrund von
Ressourcenknappheit nicht wegzudenken.

Um eine Interoperabilität der unterschiedlichen Systeme und eine gute
Auffindbarkeit der enthaltenen Ressourcen zu ermöglichen, ist eine
Standardisierung notwendig - z.B. über Zertifikate, Metadatenstandards
und Schnittstellen. Die in diesem Zusammenhang wichtigen Grundlagen
werden im folgenden Kapitel erläutert.

## Übergreifende Themen

### Zertifikate und Standards

Forschungsnahe Dienste bewegen sich von jeher an der Schnittstelle
zwischen Wissenschaft und Infrastruktureinrichtungen, die die Dienste
betreiben. Zertifikate erfüllen in diesem Spannungsfeld verschiedene
Funktionen. Sie waren als Vertrauen gebende Maßnahmen angedacht, die
Qualitätsmerkmale der Dienstleistungen sein sollten. Das
[DINI-Zertifikat für Open Access
Publikationsplattformen](https://dini.de/dienste-projekte/dini-zertifikat)
versteht sich seit jeher auch als Ratgeber bei Einrichtung,
Weiterentwicklung und Betrieb solcher Dienstleistungen, der "Maßstäbe,
Richtlinien und Best Practices" vermitteln will. Und letztlich dienen
Zertifikate auch dem Schaffen von Standards, die die Interoperabilität
der Dienste ermöglichen. Neben dem DINI-Zertifikat sind in Bezug auf
forschungsnahe Dienste auch das [Core Trust
Seal](https://www.coretrustseal.org) sowie das
[Nestor-Siegel für vertrauenswürdige digitale
Langzeitarchive](https://www.langzeitarchivierung.de/Webs/nestor/DE/Zertifizierung/nestor_Siegel/siegel.html)
zu nennen. Während es mit der DIN Norm 31644 (auch als ISO Norm 16363
verbreitet) "Information und Dokumentation - Kriterien für
vertrauenswürdige Langzeitarchive" eine offizielle Norm für die
Bewertung der Vertrauenswürdigkeit von Langzeitarchiven gibt, werden die
meisten Standards in diesem Bereich eher als Best Practices oder
Konventionen, denn als offizielle Normen eingeführt. Unabhängig von der
Frage, ob Zertifikate als vertrauensstiftend eingeschätzt werden, lohnt
es sich die Dokumentation der Zertifikate als Ratgeber oder Checkliste
zu nutzen, sowohl beim Aufbau neuer Dienste, als auch zur regelmäßigen
Überprüfung des eigenen Dienstes mit Blick auf neue Entwicklungen und
Optionen eigene Dienste weiterzuentwickeln.

Schon im [Bethesda Statement on Open Access
Publishing](http://legacy.earlham.edu/~peters/fos/bethesda.htm)
taucht das Stichwort Interoperabilität in Zusammenhang mit Repositorien
auf. Dazu gibt es verschiedene technische Ansätze (siehe unter anderem
unten "[Schnittstellen](#schnittstellen)"). Neben den technischen Voraussetzungen, um
Inhalte zu teilen, braucht es jedoch auch eine Einigung über die
inhaltliche Aufbereitung der Informationen. Repositorien nutzen dazu
strukturierte Metadaten. Für die Bezeichnung von Dokumententypen haben
die DINI AG Elektronisches Publizieren und die DINI AG
Forschungsinformationssysteme das [Gemeinsame Vokabular für
Publikations- und Dokumenttypen](https://doi.org/10.18452/24147)
herausgegeben. Im Sinne der Standardisierung enthält das DINI-Zertifikat
weitere Vorgaben, wie zum Beispiel die Klassifizierung nach zumindest
den DDC-Sachgruppen der Deutschen Nationalbibliografie und macht
Vorgaben an die Ausgestaltung der OAI-PMH-Schnittstelle. Diese Standards
ermöglichen es Diensten wie zum Beispiel der Bielefeld Academic Search
Engine und anderen Aggregatoren Inhalte aus vielen verschiedenen Quellen
einzubinden, Metadaten maschinenlesbar zu erhalten und nachzunutzen.

Referenzen

- [DINI-Zertifikat 2022 für Open Access
Publikationsdienste](https://doi.org/10.18452/24678)

- [https://www.coretrustseal.org/](https://www.coretrustseal.org/)

- [https://www.langzeitarchivierung.de/Webs/nestor/DE/Zertifizierung/nestor_Siegel/siegel.html](https://www.langzeitarchivierung.de/Webs/nestor/DE/Zertifizierung/nestor_Siegel/siegel.html)

### Metadaten

Metadaten sind Daten struktureller, technischer, administrativer,
bibliographischer und deskriptiver Natur, die Daten beschreiben (TODO: siehe
Kap. **Daten & Metadaten** bzw. *Metadatenstandards*)*.* Metadaten werden oft
in einer Schlüssel-Wert-Struktur genutzt, bei der der Schlüssel vorgibt, welche
Angabe (z. B. Titel, Autorschaft, Erscheinungsdatum, ...) im Wert zu finden ist
(siehe Kap. **Daten & Metadaten** bzw*. Grundlegende Begrifflichkeiten*).
Strukturell werden flache und hierarchische Metadatenschemata unterschieden.
Flache Metadatenschemata beschränken sich auf eine einfache Struktur aus
Schlüssel-Wert-Paaren. Hierarchische Metadatenschemata sehen vor, Werte aus
anderen Werten zusammensetzen zu können, so dass z. B. die Autorenschaft in den
Metadaten über Personen modelliert werden können und zu jeder Person Vorname,
Nachname und weitere Angaben in untergeordneten Werten gespeichert wird.
Hierarchische Metadatenschemata haben dabei oft etliche dieser Hierarchieebenen
und Verschachtelungen.

Metadatenschemata definieren, welche Inhalte in den Metadaten erfasst
werden, also welche Metadatenfelder existieren und mit Werten belegt
werden können. [Dublin] [Core
Element](https://www.dublincore.org/specifications/dublin-core/usageguide/elements/)
(umgangssprachlich oft einfach *Dublin Core* genannt) ist als
Metadatenschema für Inhalte im Internet entstanden (TODO: siehe Kap. **Daten &
Metadaten** bzw. *Metadatenstandards*) . Es wird oft in Metatags auf
HTML-Seiten verwendet und ist vorherrschend beim Austausch von Daten
zwischen und mit Repositorien, auch wenn so gut wie jede
Repositoriensoftware intern deutlich mehr Metadatenschemata unterstützt
und nutzt. Es wird zwischen Simple und Qualified Dublin Core
unterschieden. Simple Dublin Core besteht aus 15 Elementen, in Qualified
Dublin Core können ergänzend Qualifier genutzt werden. So kann zum
Beispiel das Element dc.contributor weiter spezifiziert werden als
dc.contributor.author, dc.contributor.translator,
dc.contributor.illustrator und so weiter. Von der Dublin Core Metadata
Initiative wurde später auch das Metadatenschema DCMI Terms geschaffen,
das Simple und Qualified Dublin Core zusammenfassen sollte. In der
Praxis wird nach wie vor häufig Dublin Core Elements genutzt, in
Schnittstellen oft ohne Qualifier. DCMI Terms werden in Repositorien
dann oft ergänzend zu Dublin Core Elements verwendet.

Im Rahmen der DOI-Registrierung werden auch Metadaten erhoben. Das
[DataCite Schema](https://schema.datacite.org) hat sich
dabei als ein wichtiges Metadatenschema etabliert, das zum Teil auch
losgelöst von DOIs zur Beschreibung von Forschungsdaten genutzt wird
(TODO: siehe Kap. **Daten & Metadaten** bzw. *Metadatenstandards*).

Darüber hinaus entwickeln die verschiedenen Fachdisziplinen eigene
domänenspezifische Standards (Übersicht:
<https://fairsharing.org/search?fairsharingRegistry=Standard>
). Eine Aufgabe wissenschaftlicher Bibliotheken besteht in der Beratung
bei der Auswahl geeigneter Metadatenschemata und Standards (Übersicht
unter:
<https://www.forschungsdaten.info/themen/beschreiben-und-dokumentieren/metadaten-und-metadatenstandards>-

### Persistent Identifier

Mit dem Aufkommen elektronischer Archive kam die Frage nach der
Zitierbarkeit auf. Auch wenn die technischen Protokolle, auf denen das
Internet basiert, sowohl in DNS als auch http(s) Mechanismen enthalten,
um URLs weiterzuleiten, bekamen URLs schnell den Ruf flüchtig zu sein.
Auch wenn URLs, die nicht mehr oder auf andere Inhalte auflösen, immer
auf Managementprobleme zurückgehen, wurden Persistent Identifier Systeme
geschaffen, die diese Probleme beim Zitieren elektronischer Quellen
überwinden sollen. Dabei werden IDs geschaffen, die über einen
sogenannten Resolver aufgelöst werden können. Der Resolver ist
vergleichbar mit einem Melderegister: man kann nach der aktuellen
Adresse einer ID fragen und erhält die jeweils aktuelle URL zurück,
unter der sich die Ressourcen befinden sollen. Die Antworten können also
zu unterschiedlichen Zeitpunkten unterschiedlich ausfallen.

Während die Deutsche Nationalbibliothek bis heute auf URN:NBN als
Persistent Identifier setzt, haben sich im wissenschaftlichen Umfeld
DOIs für die Identifikation von Artikeln, Daten und anderen Inhalten
durchgesetzt. In Deutschland kommen dabei vor allem die
DOI-Registrierungsagenturen
[DataCite](https://datacite.org) und
[CrossRef](https://www.crossref.org/) zum Einsatz. Beide
vergeben DOIs sowohl für textuelle Publikationen als auch für Datensätze
und andere Inhalte. Von der technischen Einbindung her ist DataCite
moderner aufgestellt und leichter zu integrieren. Mit
[ORCID](https://orcid.org/) und
[ROR](https://ror.org) gibt es inzwischen weitere
Persistent Identifier Systeme, die zunehmend Verbreitung finden und
Personen bzw. Einrichtungen eindeutig identifizieren.

Die Vergabe von PIDs für Publikationen (z. B. Texte, Forschungsdaten)
auf Publikationsservern bzw. Datenrepositorien wird teilweise von den
wissenschaftlichen Bibliotheken gewährleistet. Somit werden
Forschungsdaten nachhaltig unter entsprechenden Lizenzen öffentlich
verfügbar gemacht (Berg-Weiß et al. 2022). Mit dem Ziel, eine nationale
Beratungs- und Austauschplattform zu persistenten Identifikatoren
aufzubauen, fördert die DFG seit 2023 das Projekt "[PID Network
Deutschland](https://www.pid-network.de/) - Netzwerk für
die Förderung von persistenten Identifikatoren in Wissenschaft und
Kultur".

### Schnittstellen

Im Bereich von Repositorien hat sich das [Open Archives Initative
Protocol for Metadata
Harvesting](https://www.openarchives.org/pmh/) (OAI-PMH)
für den Austausch von Metadaten durchgesetzt. Dieses Protokoll wird
inzwischen auch im Zusammenspiel mit anderen forschungsnahen Diensten
wie zum Beispiel FIS genutzt. Das Protokoll tauscht Metadaten in XML
aus. Es unterstützt mehrere Metadatenformate, wobei die Spezifikation
von OAI-PMH nur Dublin Core vorgibt und das Protokoll vorsieht, dass man
eine Liste mit weiteren unterstützten Formaten abrufen kann.

Für das Einbringen von Daten in Repositorien hat sich das Protokoll
[Simple Webservice Offering Repository
Deposit](https://swordapp.org/) (SWORD) durchgesetzt, wobei
auf die genaue Version dieses Standards geachtet werden muss. Einige
Open-Access-Verlage bieten an, Dokumente über SWORD 1 direkt in
Repositorien zu übertragen.
[DeepGreen](https://info.oa-deepgreen.de/), ein
Lieferdienst für Open-Access-Artikel, versorgt Repositorien über *SWORD*
mit Verlagsinhalten.

Speziell für die Arbeit mit Bildern, Bildviewern und Bilddatenbanken
wurde das *International Image Interoperability Framework (IIIF)*
entwickelt. IIIF deckt umfangreich verschiedene Funktionen ab, wie die
Ausgabe von Bildern in verschiedenen Formaten und Auflösungen oder
Zoomstufen, die strukturelle Beschreibung von Bildern, Suchanfragen in
einem Bildpool, Umgang mit Zugriffsbeschränkungen, Objektänderungen und
so weiter.

OAI-PMH und SWORD werden zum Teil zwar auch über Repositorien hinaus
verwendet, zum Beispiel bei Forschungsinformationssystemen. Verbreitung
und Einsatz beschränken sich jedoch weitestgehend auf den Bereich der
forschungsnahen Dienste. Als weiter verbreitete Prinzipien zur
Bereitstellung von Informationen muss Linked Data gesehen werden. Die
Bereitstellung der Repositorieninhalte als Linked Data wird zum Beispiel
von DSpace unterstützt, in der Praxis jedoch leider noch viel zu selten
aktiviert und genutzt.

Allgemein verbreiten sich derzeit REST-Schnittstellen, also
Schnittstellen zur Einbindung von Diensten über das Internet in
verschiedene Programme und Infrastrukturen. Auch forschungsnahe Dienste
profitieren sehr von der Bereitstellung von REST-Schnittstellen, damit
sie miteinander verschränkt und in andere Infrastrukturen eingebunden
werden können.

Im Bereich von Repositorien, der in Bezug auf Schnittstellen und
Standards oft Auswirkungen auf andere forschungsnahe Dienstleistungen
hat, wurden von der [Coalition of Open Access
Repositories](https://www.coar-repositories.org/) (COAR)
weitere neue Protokolle vorgeschlagen. Hierzu zählt Signposting, das
typisierte Links einsetzt, um von einer Ressource auf andere in
Verbindung stehende Ressourcen zu verlinken und so die Auffindbarkeit
durch Crawler und Bots erleichtern soll. Zum Zeitpunkt der Entstehung
dieses Buchs ist offen, ob Signposting sich durchsetzt, wenn möglich
sollte es in Repositorien aktiviert werden.

Eine ganz aktuelle Entwicklung ist das [Notify
Project](https://www.coar-repositories.org/notify/), das
ebenfalls von COAR betrieben wird. Notify soll es ermöglichen, dass
verschiedene forschungsnahe Dienste sich Nachrichten über Aktivitäten
senden und so auf Ressourcen aufmerksam machen. Als Beispiele gelten
automatisiert ablaufende Aktivitäten zwischen einem textuellen
Repositorium und einem Forschungsdatenrepositorium oder zwischen einem
Repositorium und einem Service für die Organisation des Peer-Reviewing.
Notify hat vor allem in den USA eine größere Förderung erhalten und wird
derzeit in Softwarelösungen für verschiedene forschungsnahe Dienste
integriert. Zum Zeitpunkt des Schreiben dieses Buchs ist zu erwarten,
dass Notify sich in diesem Bereich verankern und helfen wird, Dienste
dynamischer miteinander zu verknüpfen.

*Von diesen Entwicklungen wird auch die Veröffentlichung von
Forschungsdaten profitieren. Bevor Daten allerdings in einem Zustand
sind, dass sie veröffentlicht werden können, durchlaufen sie einen
eigenen Lebenszyklus, bei dem unterstützende Dienste der Bibliotheken
zunehmend gefragt sind.*

### Toolchain

Forschungsnahe Dienste müssen immer im Kontext des Forschungsprozesses
und im Hinblick auf den Nutzen für die Forschenden betrachtet werden.
Dazu ist es wichtig, für jeden der Dienste, die an einer Einrichtung
genutzt werden, ein klares Profil zu erstellen. Hierbei muss
insbesondere geklärt werden, wie sich die Dienste voneinander abgrenzen,
damit Nutzer\*innen klar kommuniziert werden kann, welcher Dienst wofür
verwendet wird.

Gleichzeitig muss aber auch das Zusammenspiel der einzelnen Dienste
analysiert werden. Wie knüpfen die verschiedenen Dienste aneinander an?
Wie kann der gebotene Mehrwert durch Verknüpfungen der Dienste
gesteigert werden? Und welche Dienste bauen wie aufeinander auf? Diese
Fragen sind nur lokal und konkret zu vorhandenen oder in Planung
befindlichen Diensten zu beantworten.

Beispiel: An einer Bibliothek findet in Zusammenarbeit mit
Fachwissenschaftler\*innen ein großes Retro-Digitalisierungsprojekt statt. Für
die Verwaltung der [Digitalisierungsvorgänge](#digitalisierung) wird *Kitodo*
verwendet. Die Werke sind im Bibliotheksmanagementsystem verzeichnet und die
Metadaten werden über die SRU-Schnittstelle nach Kitodo importiert. Weitere
Strukturdaten werden in Kitodo direkt eingetragen. Nach dem Scannen werden die
Dokumente von Kitodo über die REST-API oder SWORD ins Repositorium exportiert.
Das Repositorium ruft weitere Metadaten über SRU aus dem Bibliothekskatalog ab
und vergibt DOIs, die auch wieder in das Bibliotheksmanagementsystem
zurückgespeichert werden. Das Forschungsinformationssystem harvestet die
Inhalte über OAI-PMH regelmäßig und weist die Digitalisate nach, die im
Repositorium bereitgestellt werden.

*In diesem ideellen Bild wird nicht betrachtet, wo die nötigen Systeme
stehen. Dies muss nicht immer ein lokal betriebenes System auf eigener
Hardware sein, sondern ist oftmals als externer Service verfügbar.*

### Zusammenarbeit mit Dienstleistern

Selbstverständlich ist es nicht bei allen Problemstellungen möglich,
IT-Services für forschungsnahe Dienste im eigenen Haus anzubieten. Bei
bereits etablierten Anwendungen lohnt sich eine Kontaktaufnahme mit der
jeweiligen Verbundzentrale. Häufig werden dort bereits Services
angeboten, für die man kein zusätzliches Personal bzw. keine eigene
Infrastruktur einplanen muss. Beispielsweise sind das der
Repository-Service
[Reposis](https://www.gbv.de/informationen/Verbundzentrale/serviceangebote/reposis-repository-service)
des GBV oder das Langzeitarchiv
["Ewig"](https://ewig.zib.de/) des KOBV.

Wird die Verwendung einer bestimmten Software gefordert oder eine
bestimmte Art, die Software einzusetzen, die nicht im
Dienstleistungsportfolio der Verbundzentralen liegt, bietet sich die
Zusammenarbeit mit externen, kommerziellen Dienstleistern an. Wo immer
möglich, sollte Software von Open-Source-Communities Vorrang vor
proprietärer Software genießen, da sie vor Abhängigkeiten von einzelnen
Anbietern schützt und Weiterentwicklungen, die in die Community
eingebracht werden, von anderen Einrichtungen wiederverwendet werden
können. Vor allem im Bereich der forschungsnahen Dienste sind
Open-Source-Lösungen oft vorherrschend. Ziel von
Infrastruktureinrichtungen sollte es sein das so zu erhalten, anstatt
den Aufbau proprietärer Software und neuer Oligopol- oder
Monopolstellungen zuzulassen. Der Mehrwert, den IT-Dienstleister bieten,
besteht ähnlich wie bei den Verbundzentralen darin, dass man selbst
personelle und ggf. Infrastrukturressourcen sparen kann. In den meisten
Fällen gibt es die Möglichkeit, entweder die Installation und Betreuung
des Dienstes auf lokaler Infrastruktur oder auch ein
"Rundum-Sorglos-Paket" mit Hosting beim Dienstleister inklusive
Betreuung einzukaufen.

Bei der Wahl des Anbieters gilt es, auf dessen Erfahrung im Umgang mit
Open-Source-Projekten allgemein und mit der gewünschten Software im
Speziellen zu achten. Genau wie nach Referenzen zu vergleichbaren
Projekten zu fragen, lohnt es sich nach konkreten bereits geleisteten
Beiträgen zu der jeweiligen Open-Source-Lösung zu fragen. Wenn die
Open-Source-Lizenzen der jeweiligen Software keine Auflage machen, dass
Weiterentwicklungen unter derselben Lizenz verbreitet werden müssen,
sollte die Frage, unter welcher Softwarelizenz Weiterentwicklungen
stehen, zwingend im Vertrag geklärt werden.

Besonders weit verbreitete Softwarelösungen haben große
Entwicklergemeinschaften, die es einerseits zu unterstützen gilt,
andererseits aber auch die communitygetriebene Entwicklungsrichtung der
Software zu beachten ist. Ist der gewünschte Dienstleister hier noch
neu, ist zumindest eine anderweitige Erfahrung in ähnlichen Projekten
wünschenswert. Eine Recherche in öffentlichen Software-Repositorien der
Projekte kann hier zielführend sein.

## Zusammenfassung und Ausblick

Forschungsnahe Dienste können Wissenschaftler\*innen grundsätzlich über
den gesamten Forschungsprozess hinweg unterstützen - das Portfolio
möglicher Services ist daher sehr groß. Bibliotheken setzen ihren
Schwerpunkt hierbei insbesondere auf Services zur Unterstützung des
Publikationsprozesses sowie des Forschungsdatenmanagements.

Wie das vorliegende Kapitel anschaulich gezeigt hat, umfassen diese
Services auch eine Vielzahl an IT-Diensten, so z. B.
Journal-Publishing-Systeme, Repositorien und
Forschungsinformationssysteme. Der stabile und nachhaltige Betrieb
solcher Dienste umfasst technische, organisatorische und inhaltliche
Aspekte. Der Aufbau und Betrieb forschungsnaher Dienste an Bibliotheken
bindet daher umfangreiche Ressourcen und erfordert ggf. auch eine
Verlagerung von Ressourcen aus anderen Bereichen. Die Ausweitung des
bibliothekarischen Serviceportfolios um forschungsnahe Dienste ist daher
derzeit vor allem auch eine Frage der Organisations- und
Personalentwicklung.

Zur vertieften Beschäftigung mit forschungsnahen Diensten an
Bibliotheken wird folgende Literatur empfohlen:

-   Konrad, Uwe, Förstner, Konrad, Reetz, Johannes, Wannemacher, Klaus,
    Kett, Jürgen, & Mannseicher, Florian (2020). Digitale Dienste für
    die Wissenschaft. Wohin geht die Reise? (Positionspapier), Zenodo.
    <https://doi.org/10.5281/zenodo.4301924>

-   Farrenkopf et al. (2021). Forschungsunterstützung an Bibliotheken:
    Positionspapier der Kommission für forschungsnahe Dienste des VDB.
    O-Bib. Das Offene Bibliotheksjournal / Herausgeber VDB, 8(2),
    1--19.
    <https://doi.org/10.5282/o-bib/5718>

-   Berg-Weiß, A., et al. (2022). Openness in Bibliotheken :
    Positionspapier der Kommission für forschungsnahe Dienste des VDB.
    O-Bib. Das Offene Bibliotheksjournal / Herausgeber VDB, 9(2),
    1--4.  <(https://doi.org/10.5282/o-bib/5826>

-   Azeroual, Otmane. "Untersuchungen zur Datenqualität und
    Nutzerakzeptanz von Forschungsinformationssystemen". Dissertation.
    Magdeburg: Otto-von-Guericke-Universität Magdeburg, 2021.

-   Schirrwagen, Jochen: Repositorien und Forschungsinformationssysteme
    bilden keine Dichotomie, Bibliothek - Forschung und Praxis 46, 2, 2022, 284--288.

-   Management von Forschungsinformationen in Hochschulen und
    Forschungseinrichtungen. Eine Standortbestimmung 2022 (2022),
    hrsg. DINI AG Forschungsinformationssysteme,
    <https://doi.org/10.18452/25440>

-   Druskat, Stephan und Bertuch, Oliver und Juckeland, Guido und
    Knodel, Oliver und Schlauch, Tobias (2022) *Software publications
    with rich metadata: state of the art, automated workflows and
    HERMES concept.* sonstiger Bericht. doi:
    <https://doi.org/10.48550/arXiv.2201.09015>;
    mith AM, Katz DS, Niemeyer KE, FORCE11 Software Citation Working
    Group. 2016. Software citation principles. PeerJ Computer Science
    2:e86
    <https://doi.org/10.7717/peerj-cs.86>

-   Yves Vincent Grossmann und Michael Franke, Software ist kein
    Beiprodukt! Nachhaltige Forschungssoftware durch
    Software-Management-Pläne, b-i-t-online 26 (2023) Nr. 5, 457- 463,
    <https://www.b-i-t-online.de/heft/2023-05-fachbeitrag-grossmann.pdf>

-   Cyra, M. A., Politze, M. ., & Timm, H. (2022). A push for better
    RDM: Erfahrungsbericht aus dem Einsatz von git für
    Forschungsdaten. *Bausteine Forschungsdatenmanagement*, (2).
    <https://doi.org/10.17192/bfdm.2022.2.8435>

-   Barker, M., Chue Hong, N.P., Katz, D.S. *et al.* Introducing the
    FAIR Principles for research software. *Sci Data* **9**, 622
    (2022). <https://doi.org/10.1038/s41597-022-01710-x>

-   Putnings, Markus, Neuroth, Heike und Neumann, Janna. *Praxishandbuch
    Forschungsdatenmanagement*, Berlin, Boston: De Gruyter Saur, 2021.
    <https://doi.org/10.1515/9783110657807>
